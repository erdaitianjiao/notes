### An Interface-Aware Cache Management Framework for Task Partitioning on CPU-FPGA SoC Platforms

TAPCA：一种面向接口感知缓存管理的CPU-FPGA SoC平台任务划分框架

#### 背景

> - HSAs（Heterogeneous System Architectures，异构系统架构）
>
>   > **一种计算架构**，它将不同类型的计算单元（如CPU、GPU、FPGA等）集成到一个系统中<br>以充分利用每种计算单元的优势，从而提高系统的整体性能和能效
>   
> - 传统CPU-FPGA HSA的问题
>
>   > **基于PCIe通信和分离式内存架构**<br>CPU常将整个计算任务卸载到FPGA，因通信开销大，CPU多处于空闲状态，且工作负载不平衡会阻碍FPGA优化
>
> - CPU-FPGA SoC
>
>   > **CPU和FPGA被集成在同一芯片上**，形成一个完整的系统<br>这种集成方式使得两者之间的通信更加高效，减少了传统系统中由于物理距离和接口限制带来的通信延迟和带宽问题
>
> - 共享内存架构的优势
>
>   > 共享内存架构是一种在多处理器系统中，允许多个处理器或协处理器（如CPU和FPGA）访问同一块内存的技术<br>这种架构的主要目的是减少数据传输的开销，提高系统整体的性能和效率



#### 基础知识

> - 重用距离（Reuse Distance）
>
>   > 一个用于衡量内存访问模式中数据局部性的指标。它定义为从一个内存访问到其最近一次访问之间访问的不同内存位置的数量。重用距离用于：
>   >
>   > - **内存管理建模**：通过分析每个划分单元（PU）的重用距离，评估其缓存利用效率。具体来说，重用距离直方图（RDH）生成器会生成每个PU的重用距离分布，帮助选择最适合的内存架构。
>   > - **选择合适的内存架构**：根据重用距离的分布，选择最适合的内存架构（如L1S、FCA、LLCS等），以减少通信开销。例如，如果一个PU的重用距离较短，说明其数据局部性较好，适合使用缓存一致性架构（如FCA）；如果重用距离较长，说明数据局部性较差，适合使用缓存旁路架构（如LLCS）。
>
> - 缓存旁路（Cache Bypassing）
>
>   > 一种优化技术，允许某些内存访问直接绕过缓存，直接访问内存。在论文中，缓存旁路机制用于：
>   >
>   > - **减少缓存不命中开销**：对于重用距离较长的PU，缓存命中率较低，缓存旁路可以避免缓存不命中带来的性能损失。
>   > - **选择合适的内存架构**：通过缓存旁路机制，可以选择最适合PU的内存架构。例如，对于重用距离较长的PU，启用缓存旁路，直接访问内存，避免缓存不命中带来的性能损失。
>
> - **CPU-FPGA SoC中的缓存一致性：**
>
>   > 主要的共享内存架构包括CPU L1缓存共享架构（L1S）、全一致性架构（FCA）、CPU最后一级缓存共享架构（LLCS）和DDR共享架构（DDRS）。这些架构通过不同的方式促进CPU-FPGA之间的通信。



#### 主要内容

> -  **TAPCA Framework**
>
>   > - TAPCA（Task Partitioning and Cache Allocation）框架是一种用于优化CPU-FPGA异构系统中应用程序性能的自动化工具。它的目标是通过合理划分任务（Partitioning）和分配缓存资源（Cache Allocation），找到最佳的软硬件划分方案，从而最大化系统的整体性能。以下是对TAPCA框架的详细解释：
>   >
>   >   #####  概述
>   >   TAPCA框架通过五步流程处理C/C++应用程序的LLVM IR基本块（BBs），以确定最佳的分区方案，并将关键路径上的处理单元（PUs）与它们的最佳内存架构相匹配。具体步骤如下：
>   >
>   >   ###### 第一步：生成分区单元候选（PUCs）
>   >   - **输入**：C/C++程序的LLVM IR基本块（BBs）。
>   >   - **处理**：分析关键路径上BBs之间的控制流信息，识别出程序中的基本结构，如子循环、循环、分支或整个函数。
>   >   - **输出**：生成一组分区单元候选（PUCs），每个PUC代表程序中一个可以独立执行的代码片段。
>   >
>   >   ###### 第二步：设计空间探索（DSE）
>   >   - **输入**：每个PUC。 
>   >   - **处理**：对每个PUC的多个设计点进行探索，通过表1中的五个优化指令，评估每个设计点的硬件执行时间（$(HT_{ij}$）和资源使用量（$R{ij}$）。
>   >   - **输出**：为每个PUC生成多个设计点的性能数据，包括硬件执行时间和资源使用量。
>   >
>   >   ###### 第三步：通信开销建模
>   >   - **输入**：PUCs的设计点性能数据。
>   >   - **处理**：通过分析内存访问模式的重用距离直方图（RDH），对每个设计点的通信开销（$CT_{ij}$）进行建模。根据内存访问模式，选择合适的缓存旁路策略和内存接口，并评估所选内存架构的开销。
>   >   - **输出**：为每个PUC的每个设计点生成完整的性能配置文件，包括软件执行时间（$ST_i$）、硬件执行时间（$HT_{ij}$）、资源使用量（$R_{ij}$）和通信开销（$CT_{ij}$）。
>   >
>   >   ###### 第四步：生成分区单元方案（PUSs）
>   >   - **输入**：PUCs及其性能配置文件。
>   >   - **处理**：将互斥的PUCs组合成多个分区单元方案（PUSs），每个PUS代表一种可能的程序划分方式。这些PUSs涵盖了不同的软硬件划分策略。
>   >   - **输出**：生成多个PUSs，每个PUS包含一组PUCs及其对应的性能配置文件。
>   >
>   >   ###### 第五步：关键路径选择（KPS）
>   >   - **输入**：多个PUSs及其性能配置文件。
>   >   - **处理**：将这些PUSs输入到关键路径选择器（KPS）中，通过综合评估每个PUS的性能，选择最优的分区方案。最优方案将指明哪些PUs在CPU上执行（软件PUs），哪些PUs在FPGA上执行（硬件PUs），并提供硬件PUs的设计空间探索和内存接口配置。
>   >   - **输出**：确定最佳的分区方案，用于配置Vivado块设计。PetaLinux使用Vivado生成的硬件描述文件生成最终的可执行文件。
>   >
>   >   ##### TAPCA框架的关键点
>   >   1. **自动化**：TAPCA框架通过自动化的方式处理程序的LLVM IR基本块，减少了手动优化的复杂性。
>   >   2. **性能优化**：通过设计空间探索（DSE）和通信开销建模，TAPCA能够找到最优的软硬件划分方案，最大化系统的整体性能。
>   >   3. **缓存管理**：通过分析内存访问模式和选择合适的缓存策略，TAPCA能够有效减少通信开销，提高缓存利用率。
>   >   4. **灵活性**：TAPCA支持多种内存架构（如L1S、FCA、LLCS等），适用于不同的CPU-FPGA SoC架构。
>   >
>   >   
>   >   
>   > ![share men](.\img\share men.png)
>   >   
>   >   
>   >
>   > ![11](.\img\TAPCA.png)
>
> - **TAPCA框架中的PUCs和PUSs生成过程**
>
>   >###### 1. **PUCs（Partitioning Unit Candidates，分区单元候选）**
>   >- **定义**：PUCs是程序中可以独立执行的代码片段或模块，代表应用程序中的一个功能段。
>   >- **生成方法**：
>   >  - 使用`getPuCandidates()`函数，根据基本块（BBs）之间的控制依赖（𝐶𝑡𝑟𝑙𝐷𝑒𝑝）将关键路径上的BBs聚类成PUCs。
>   >  - 控制依赖计算公式：如果BBs $𝑏𝑖$ 和 $𝑏𝑗$ 之间有 $\beta$ 个控制流，它们的执行频率分别为 $𝑛𝑖$ 和 $𝑛𝑗$，则 $𝐶𝑡𝑟𝑙𝐷𝑒𝑝(𝑏𝑖, 𝑏𝑗) = 𝑛𝑖 \times 𝑛𝑗 \times \beta$。
>   >  - 当 $𝐶𝑡𝑟𝑙𝐷𝑒𝑝(𝑏𝑖, 𝑏𝑗)$ 超过阈值 $\tau_{cd}$ 时，$𝑏𝑖$、$𝑏𝑗$ 以及它们之间的所有BBs可以被组合成一个PUC。
>   >- **作用**：
>   >  - PUCs是KPS（关键路径选择器）的基础组件，用于后续的设计空间探索（DSE）和性能评估。
>   >  - PUCs的大小可以根据程序的结构灵活调整，从基本块到函数级别不等。这种可调节的划分粒度相比粗粒度（函数级别）任务划分能更好地平衡工作负载，同时与细粒度（指令级别）划分相比，减少了划分时间和CPU-FPGA通信开销。
>   >
>   >###### 2. **PUSs（Partitioning Unit Schemes，分区单元方案）**
>   >- **定义**：PUSs是一种高层次的设计策略，描述了如何将PUCs划分为多个组，并决定每个组在硬件（如FPGA）或软件（如CPU）上的执行方式。每个PUS代表了一种可能的程序划分方案。
>   >- **生成方法**：
>   >  - 在第3.3节获得所有PUCs及其配置文件元组后，使用`getPuSchemes()`函数将PUCs组合成各种配置过的PUSs。
>   >  - 每个PUS $𝑈𝑗$ 中的PUC $𝑐𝑖$ 都满足条件：$\forall 𝑐𝑖 \in 𝑈𝑗, 𝑐𝑖 = 𝐵𝐵$，且 $𝑈𝑗$ 中不存在空的PUC，其中 $𝐵𝐵$ 表示应用程序中所有BBs的集合。
>   >- **作用**：
>   >  - PUSs提供了多种可能的程序划分方案，使得优化算法可以从中选择最优的方案。
>   >  - 每个PUS都有一个配置文件，记录了该方案下各个PUCs的执行时间、资源使用量和通信开销等信息。通过评估这些配置文件，可以确定最优的划分方案。
>   >
>   >##### 总结
>   >- **PUCs** 是程序中可以独立执行的代码片段，通过控制依赖分析生成，是KPS的基础组件。
>   >- **PUSs** 是一种高层次的设计策略，描述了如何将PUCs划分为多个组，并决定每个组的执行方式。
>   >- **可调节的划分粒度** 使得TAPCA框架能够更好地平衡工作负载，减少划分时间和通信开销。
>   >- **PUSs** 提供了多种可能的划分方案，为优化算法选择最优方案提供了基础。
>   >
>   >通过合理生成PUCs和PUSs，TAPCA框架能够有效地优化程序在CPU-FPGA异构系统中的执行效率。
>
> - **设计空间探索（DSE）用于PU候选**
>
>   > - **目标**：为每个PUC选择多个设计点，生成涵盖延迟（latency）和资源使用量的设计空间探索（DSE）结果。
>   > - **方法**：
>   >   - 使用先前的工作[17]来生成这些结果。
>   >   - 在循环展开（loop unrolling）中，用 $ \log_2 $ 表示循环边界（Loop Bounds，$ LB $），以确保在合理的设计点数量内获得全面的建模结果。
>   >   - 对于数组分区（Array Partitioning），由于CPU-FPGA内存接口的位宽 $ BM $ 是固定的，最大分区因子设置为 $ BM / BD $，其中 $ BD $ 表示数组数据的位宽。
>   >   - 表1列出了TAPCA中PUC的设计点，包括数据流（Dataflow，DF）、函数流水线（Function Pipeline，FP）、循环流水线（Loop Pipeline，LP）、循环展开（Loop Unroll，LU）和数组分区（Array Partition，AP）的配置及其对应的设计点数量。
>   >
>
>  - **内存管理建模** 
>
>    >- **目标**：为了减轻CPU-FPGA协作中组件间通信的开销，TAPCA通过一个内存管理子框架 $ M $ 建模应用程序的内存行为，选择最适合的共享内存架构和接口，并为每个PUC建模所选内存架构的通信开销，为第3.4节中的关键路径选择器（KPS）做准备。
>    >- **方法**：
>    >- **基于缓存旁路的内存架构选择器**：
>    >  
>    >  - 首先，收集每个PUC的内存访问痕迹以计算重用距离 $ d $，这通过LLVM Passes完成。
>    >  - 收集到的内存访问痕迹被输入到重用距离直方图（Reuse Distance Histogram，RDH）生成器中，为每个PUC生成RDH，如图3所示。
>    >  - 基于RDH，根据缓存旁路机制为每个PUC分配合适的内存架构和接口。
>    >  - 在选择内存架构之前，做了两个假设以确保选择过程顺利：
>    >    1. L1S的数据局部性分析基于CPU LLC（Last-Level Cache，最后一级缓存）的关联性 ACL 。这是因为FPGA访问CPU L1缓存和LLC的带宽几乎相同[10]。
>    >    2. FCA（Full Coherency Architecture，全一致性架构）中FPGA一致性缓存的关联性 $ AF $ 通常小于 $ ACL $。这是因为靠近计算单元的缓存通常使用较低的关联性以减少命中延迟[11]。
>    >  - 定义了内存架构选择器 $S(H_i) $（公式1），用于识别适合PUC$ i $ 的合适一致性缓存架构。其中 $H_i(d) $表示重用距离为 $ d $ 的引用次数，$ N_i $ 表示由PUC $ i $ 生成的总内存访问次数，$ \tau $ 是一个可调阈值，用于分析PUC $ i $ 的数据局部性。
>    >  - 选择器 $ S(H_i) $ 指出，对于具有关联性 $ A $ 的一致性缓存架构，要适合PUC $ i $，必须满足两个条件：（1）必须有 $ d < A $ 的内存访问以确保缓存命中；（2）这些内存访问的比例不应被忽视（不低于阈值 $ \tau $ [4]）。
>    >- **通信开销建模**：
>    >  
>    >  - 对于CPU-FPGA通信过程，假设存在 $ N_b $ 个分离的突发传输（bursts），其中 $ I_k $ 表示突发 $ k $ 和突发 $ k+1 $ 之间的发布间隔，$ \theta_i $ 表示单个突发 $ i $ 的传输开销，$ \Theta_i $ 表示从突发1的发布到突发 $ i $ 完成的总传输开销。
>    >  - 由于CPU-FPGA SoC中的所有内存架构都是基于AXI的，因此可以使用[10]中量化的AXI数据传输机制来建模CPU-FPGA通信。
>    >  - AXI协议支持未完成的事务，允许在等待前一个事务完成之前发布新事务。此外，所有AXI事务都包括固定的延迟 $ L $ 和数据传输过程，因此AXI事务被建模为图4所示。
>    >  - 在图4a中，条件 $ I_1 < \theta_1 - L $ 表示数据传输2将在数据传输1完成后立即开始，其中 $ L $ 是基于AXI的传输的初始化延迟。
>    >  - 相反，在图4b中，由于 $ I_1 > \theta_1 - L $，数据传输2在数据传输1完成时尚未准备好。
>    >  - 事务1和2的总传输时间可以统一表示为 $ \Theta_2 = \max(\theta_1 + \theta_2 - L, I_1 + \theta_2) = \theta_2 + \max(\theta_1 - L, I_1) $。
>    >  - 然后，可以将这种情况扩展到 $ N_b $ 个突发事务，推导出公式（2）以计算突发 $ i $ 的总传输开销 $ \Theta_i $。
>    >  - $ N_b $ 和 $ I_k $ 的值可以使用LLVM Passes分别对连续内存地址的切片和切片之间的延迟进行分析来建模。
>    >  - 对于 $ \theta_i $，可以使用[10]中的AXI传输延迟方程（公式3）来建模：
>    >    $
>    >    \theta_M(D) = \frac{BM(D) \cdot D}{BM(D) \cdot L_M + D}
>    >    $
>    >    其中，$ \theta_M(D) $ 表示在内存架构 $ M $ 中传输大小为 $ D $ 的数据量在CPU和FPGA之间的通信开销，$ BM(D) $ 表示在传输大小为 $ D $ 的数据时内存架构 $ M $ 的带宽，$ L_M $ 是 $ M $ 的初始化延迟。
>    >  
>    >  ![bag](.\img\bag.png)
>    >
>
>  - 第5步：关键路径选择器（KPS）
>
>    >
>    >1. **问题建模**：
>    >   - **目标**：将任务分区问题建模为一个背包问题（Knapsack Problem，KP），因为它们在形式上有相似性[14][8]。
>    >  - **方法**：使用动态规划（Dynamic Programming，DP）方法求解KP模型。
>    > 
>    > 2. **KP模型的描述**：
>    >  - **关键路径**：应用程序的关键路径由一系列顺序执行的处理单元（PUs）组成，记为 $ U $，其中PUs的数量为 $ n $。
>    >  - **分区决策**：每个PU $ u_i \in U $ 需要被划分为软件组件或硬件组件。
>    >   - **通信开销**：PU $ u_i $ 和 $ u_j $ 之间的通信开销用 $ c_{ij} $ 表示，由内存管理模型 $ M $ 评估。
>    >   - **执行时间**：
>    >     - 当 $ u_i $ 被划分为软件组件时，其执行时间为 $ t_s^i $。
>    >     - 当 $ u_i $ 被划分为硬件组件时，其执行时间为 $ t_h^i $。
>    >   - **二进制变量**：引入一个二进制变量 $ \alpha_i \in \{0, 1\} $ 来表示 $ u_i $ 的实现方式，其中 $ \alpha_i = 1 $ 表示软件实现，$ \alpha_i = 0 $ 表示硬件实现。
>    >   - **总执行时间**：关键路径上的PUs顺序执行，因此总执行时间 $ T(U) $ 是所有PUs的软件执行时间、硬件执行时间和它们之间的通信开销之和，公式如下：
>    >     $
>    >     T(U) = \sum_{i=1}^{n} (1 - \alpha_i) t_s^i + \sum_{i=1}^{n} \alpha_i t_h^i + \sum_{i=1}^{n} \sum_{j=1}^{n} \alpha_i \alpha_j c_{ij} \quad (4)
>    >     $
>    > 
>    > 3. **多约束背包问题（MCKP）模型**：
>    >  - **目标**：最小化总执行时间 $ T(U) $。
>    >  - **约束条件**：
>    >     - 硬件组件的总资源限制为 $ R $。
>    >     - 每个PU $ u_i $ 的资源需求为 $ r_h^i $。
>    >     - 二进制变量 $ \alpha_i $ 的取值为0或1。
>    >   - **模型公式**：
>    >     $
>    >     \text{minimize } T(U)
>    >     $
>    >     $
>    >     \text{subject to } \sum_{i=1}^{n} \alpha_i r_h^i \leq R, \quad \alpha_i \in \{0, 1\} \quad (5)
>    >     $
>    > 
>    > 4. **求解方法**：
>    >  - 使用动态规划（DP）方法求解KP模型，以获得最优的分区结果。
>    >

#### 评估

> - 1. **评估维度**
>
>    >1.1 **可扩展性（Scalability）**
>    > 
>    > - **定义**：评估TAPCA在不同应用规模下快速提供准确解决方案的能力。
>    > - **指标**：速度（Speed），即从输入到后端（backend）的步骤执行时间。
>    > - **对比方法**：
>    >  - 使用gem5（一个全系统模拟器）和Xilinx Vitis HLS工具的全模拟方法。
>    >  - TAPCA避免了全模拟，因此在处理大型应用时速度更快，这是TAPCA的一个关键优势。
>    >
>    > 1.2 **分区性能（Partitioning Performance）**
>    >
>    > - **定义**：评估TAPCA指导下的分区实现的质量提升。
>    > - **评估方法**：
>    >  - 分析分区实现中的执行延迟时间分解，展示整体加速比（speedup）。
>    >  - 强调TAPCA在减少通信开销方面的改进。
>    >
>
>
> -  2. **实验设置**
>
>    > 2.1 **基准测试**
>    >- **工具**：使用Polybench [9]中的五个基准测试。
>    >- **对比框架**：TAPCA与改进的Wibheda+（m-Wibheda+）[15][16]进行对比。
>    >  - m-Wibheda+是一个最先进的CPU-FPGA任务分区框架，具有灵活的粒度和非一致性内存。
>    >- **硬件平台**：ZCU102开发板，一个CPU-FPGA多处理器片上系统（MPSoC）。
>    >  - **CPU**：4核Cortex-A53。
>    >  - **FPGA**：可编程逻辑，包含2520个DSP切片、548K个FFs（Flip-Flops）、274K个LUTs（Look-Up Tables）和4MB BRAM（Block RAM）。
>    >  - **内存架构**：支持所有四种内存架构，每个架构都有一个固定的128位接口。
>    >
>
>
> - 3. **评估目标**
>   >- **可扩展性评估**：通过对比TAPCA与全模拟方法的速度，展示TAPCA在处理大型应用时的效率优势。
>    >- **分区性能评估**：通过分析执行延迟的时间分解，展示TAPCA在减少通信开销和提升整体性能方面的效果。
>    >
>    > 4. **实验环境**
>    >- **硬件平台**：ZCU102开发板，一个功能强大的CPU-FPGA异构系统。
>    >- **软件工具**：Polybench基准测试、TAPCA框架和m-Wibheda+框架。
>    >
>
> - **评估结果**
>
>   >**TAPCA的可扩展性（Scalability）**
>   >
>   >- **评估方法**：使用全模拟（full simulation）方法与TAPCA的分区速度进行对比。
>   >- **结果**：
>   >  - 对于小型基准测试（如小规模GEMM），TAPCA的速度与全模拟相当。
>   >  - 对于大型基准测试（包含更多的PUCs、PUSs和更大的数据量），TAPCA显著优于全模拟，展示了其在不同应用规模下快速提供准确解决方案的能力。
>   >
>   >**分区性能和分析（Partitioning Performance and Analysis）**
>   >
>   >- **评估方法**：
>   >  - 表2展示了TAPCA生成的分区实现，包括硬件PUCs的合适内存、CPU-FPGA通信量、缓存命中率和资源利用率。
>   >  - 图7展示了TAPCA、m-Wibheda+和无分区方案的执行延迟时间分解，所有基准测试都在FPGA上使用非一致性内存和BRAM执行。
>   >- **关键发现**：
>   >  1. **Finding 1**：对于具有单一完美或半完美循环嵌套的应用程序，任务分区可能不是最佳选择。表2显示，PUCs主要由循环组成，循环内的控制依赖通常比循环外的更复杂。对于只有一个这样的循环嵌套的应用程序，划分多个PUCs可能很困难，从而降低了任务分区的有效性。数据分区[7]可能是处理这类应用的更合适选择。
>   >  2. **Finding 2**：任务分区可能在有限资源的FPGA平台上提升执行时间。如图5所示，当FPGA无法容纳应用程序的完整指令优化时，会出现不完全FPGA优化（即无分区）与改进的FPGA优化加上额外的CPU-FPGA通信开销（即任务分区）之间的权衡。表2和图7表明，将任务卸载到CPU可以显著减少执行时间，尽管存在额外的通信开销。
>   >  3. **Finding 3**：共享一致性缓存有助于减少通信开销，尤其是在通信量较大时。在类似Finding 2的场景中，当通信开销无法被计算重叠时，选择具有最小开销的内存解决方案可能是有益的。图7显示，与没有高效内存管理和选择的m-Wibheda+相比，TAPCA在通信开销方面平均提升了62.8%，在执行延迟方面平均减少了29.7%，并且更大的通信量可以获得更高的改进（相关性和协方差）。



#### 总结

>- **结论**
>
>   >- **TAPCA介绍**：本文介绍了TAPCA，这是一个针对CPU-FPGA SoC平台的任务分区框架，具有共享一致性缓存管理功能。TAPCA以C/C++应用的LLVM IR作为输入，并生成一个配置文件，详细说明CPU和FPGA的PUs分区方案、硬件PUs的DSE配置以及每个PU的最佳一致性内存。
>   >- **关键特性**：
>   >  - 基于DSE的特征提取，无需全模拟。
>   >  - 共享一致性缓存管理。
>   >- **性能提升**：
>   >  - 分区速度显著提升。
>   >  - 与非分区方案和m-Wibheda+相比，执行延迟分别减少了42.8%和29.7%。
>  >  - 通信开销平均提升了62.8%。
>  >- **未来工作**：计划在TAPCA中支持动态特征提取，并增加更多加速器类型，如AI Engine。
>
> - **总结**
> 
>   >- **TAPCA的可扩展性**：在处理大型应用时，TAPCA显著优于全模拟方法，展示了其快速提供准确解决方案的能力。
>   >- **分区性能**：TAPCA在减少通信开销和执行延迟方面表现出色，尤其是在通信量较大时。
>   >- **相关工作**：现有的任务分区研究主要集中在粒度和DSE上，但没有考虑CPU-FPGA SoCs上的内存管理。
>   >- **TAPCA的关键特性**：基于DSE的特征提取和共享一致性缓存管理，使其在分区速度和性能提升方面表现出色。
>   >- **未来方向**：支持动态特征提取和更多加速器类型，以进一步提升TAPCA的性能和适用性。



